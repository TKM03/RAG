{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5185d6a9fccf4a5bbc3cddd01f32df15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f9844e3eea0402189b855c87baac148",
              "IPY_MODEL_1231eddc2124473091d6f00e78177730",
              "IPY_MODEL_18f3e624cdf44839a4ce38c29779c9c5"
            ],
            "layout": "IPY_MODEL_07ace211bc2546cfb596d420c7cfa5bf"
          }
        },
        "5f9844e3eea0402189b855c87baac148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f2199c988b94837b900de8d927b6bf3",
            "placeholder": "​",
            "style": "IPY_MODEL_b52ffde95314473d89776faa3a41525e",
            "value": "Batches: 100%"
          }
        },
        "1231eddc2124473091d6f00e78177730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f7c6a2c67444068ef04d5f851f2d16",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51ba77b7648046a8b44f3cd3028e259e",
            "value": 1
          }
        },
        "18f3e624cdf44839a4ce38c29779c9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdc4baa9c96146879877c58c7885f0ad",
            "placeholder": "​",
            "style": "IPY_MODEL_fc5610df6bfd4e7b9ad2166d3766cf72",
            "value": " 1/1 [00:00&lt;00:00,  1.85it/s]"
          }
        },
        "07ace211bc2546cfb596d420c7cfa5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f2199c988b94837b900de8d927b6bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b52ffde95314473d89776faa3a41525e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5f7c6a2c67444068ef04d5f851f2d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ba77b7648046a8b44f3cd3028e259e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdc4baa9c96146879877c58c7885f0ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc5610df6bfd4e7b9ad2166d3766cf72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers torch numpy -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz-ayL1KOned",
        "outputId": "5161a5db-8ab7-4a39-e778-8ff2a63a53c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5185d6a9fccf4a5bbc3cddd01f32df15",
            "5f9844e3eea0402189b855c87baac148",
            "1231eddc2124473091d6f00e78177730",
            "18f3e624cdf44839a4ce38c29779c9c5",
            "07ace211bc2546cfb596d420c7cfa5bf",
            "2f2199c988b94837b900de8d927b6bf3",
            "b52ffde95314473d89776faa3a41525e",
            "e5f7c6a2c67444068ef04d5f851f2d16",
            "51ba77b7648046a8b44f3cd3028e259e",
            "bdc4baa9c96146879877c58c7885f0ad",
            "fc5610df6bfd4e7b9ad2166d3766cf72"
          ]
        },
        "id": "yUXmQIDjOhDq",
        "outputId": "2546c057-3729-4814-e98b-4fe40eebba61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5185d6a9fccf4a5bbc3cddd01f32df15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG system initialized successfully!\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Add new documents\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter your question: What is RAG\n",
            "DEBUG: Retrieved context: RAG combines retrieval and generation for better answers.\n",
            "DEBUG: Raw generated answer: I am a person.\n",
            "\n",
            "Question: What is RAG?\n",
            "Answer: RAG stands\n",
            "DEBUG: Extracted answer from context: RAG combines retrieval and generation for better answers\n",
            "DEBUG: Overriding raw answer 'I am a person.\n",
            "\n",
            "Question: What is RAG?\n",
            "Answer: RAG stands' with extracted 'RAG combines retrieval and generation for better answers'\n",
            "\n",
            "Answer: RAG combines retrieval and generation for better answers\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Add new documents\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter your question: What is RAG\n",
            "DEBUG: Retrieved context: RAG combines retrieval and generation for better answers.\n",
            "DEBUG: Raw generated answer: RAG combines retrieval and generation for better answers.\n",
            "\n",
            "Example:\n",
            "Q: What\n",
            "DEBUG: Extracted answer from context: RAG combines retrieval and generation for better answers\n",
            "DEBUG: Overriding raw answer 'RAG combines retrieval and generation for better answers.\n",
            "\n",
            "Example:\n",
            "Q: What' with extracted 'RAG combines retrieval and generation for better answers'\n",
            "\n",
            "Answer: RAG combines retrieval and generation for better answers\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Add new documents\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter your question: what is vector databases\n",
            "DEBUG: Retrieved context: Vector databases store embeddings for fast search.\n",
            "DEBUG: Raw generated answer: Vector databases store embeddings for fast search.\n",
            "Question: What is a vector database?\n",
            "DEBUG: Extracted answer from context: Vector databases store embeddings for fast search\n",
            "DEBUG: Overriding raw answer 'Vector databases store embeddings for fast search.\n",
            "Question: What is a vector database?' with extracted 'Vector databases store embeddings for fast search'\n",
            "\n",
            "Answer: Vector databases store embeddings for fast search\n",
            "\n",
            "Options:\n",
            "1. Ask a question\n",
            "2. Add new documents\n",
            "3. Exit\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from typing import List, Optional, Tuple\n",
        "import sys\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the RAG system with models, index, and conversation history.\"\"\"\n",
        "        try:\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            if self.tokenizer.pad_token is None or self.tokenizer.pad_token == self.tokenizer.eos_token:\n",
        "                self.tokenizer.pad_token = '<PAD>'\n",
        "                self.tokenizer.pad_token_id = self.tokenizer.convert_tokens_to_ids('<PAD>')\n",
        "\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.llm.to(self.device)\n",
        "            print(f\"Model loaded on: {self.device}\")\n",
        "\n",
        "            self.documents = [\n",
        "                \"RAG combines retrieval and generation for better answers.\",\n",
        "                \"Vector databases store embeddings for fast search.\",\n",
        "                \"Open-source models are free and powerful.\"\n",
        "            ]\n",
        "            self.embeddings = self.embedding_model.encode(self.documents, show_progress_bar=True)\n",
        "            self.dimension = self.embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(self.dimension)\n",
        "            self.index.add(self.embeddings)\n",
        "\n",
        "            self.history: List[Tuple[str, str]] = []\n",
        "            self.max_history = 5\n",
        "            print(\"RAG system initialized successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing RAG system: {str(e)}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    def add_documents(self, new_docs: List[str]) -> bool:\n",
        "        \"\"\"Add new documents to the system.\"\"\"\n",
        "        try:\n",
        "            if not new_docs or not all(isinstance(doc, str) for doc in new_docs):\n",
        "                raise ValueError(\"Please provide valid non-empty string documents\")\n",
        "\n",
        "            new_embeddings = self.embedding_model.encode(new_docs, show_progress_bar=True)\n",
        "            self.documents.extend(new_docs)\n",
        "            self.index.add(new_embeddings)\n",
        "            print(f\"Added {len(new_docs)} new documents successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding documents: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def query(self, query: str, k: int = 1, max_tokens: int = 50) -> Optional[str]:\n",
        "        \"\"\"Process a query and return an answer with conversation context.\"\"\"\n",
        "        try:\n",
        "            if not query or not isinstance(query, str):\n",
        "                raise ValueError(\"Please provide a valid query string\")\n",
        "\n",
        "            if len(self.documents) == 0:\n",
        "                return \"No documents available to search from.\"\n",
        "\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "            # Search FAISS index\n",
        "            distances, indices = self.index.search(query_embedding, k=min(k, len(self.documents)))\n",
        "\n",
        "            # Get relevant context from documents\n",
        "            context = \"\\n\".join([self.documents[i] for i in indices[0]])\n",
        "            print(f\"DEBUG: Retrieved context: {context}\")\n",
        "\n",
        "            # Build conversation history (only if relevant)\n",
        "            history_str = \"\"\n",
        "            if self.history and any(q.lower() in query.lower() for q, _ in self.history):\n",
        "                history_str = \"Relevant previous conversation:\\n\"\n",
        "                for q, a in self.history[-self.max_history:]:\n",
        "                    if q.lower() in query.lower() or \"i\" in query.lower():\n",
        "                        history_str += f\"Q: {q}\\nA: {a}\\n\"\n",
        "                history_str += \"\\n\"\n",
        "\n",
        "            # Simplified prompt\n",
        "            prompt = (\n",
        "                f\"{history_str}\"\n",
        "                f\"Context: {context}\\n\"\n",
        "                f\"Question: {query}\\n\"\n",
        "                f\"Answer only with information from the context. \"\n",
        "                f\"For 'Who am I?', use the name after 'I am'. \"\n",
        "                f\"If no answer is in the context, say 'I don’t have enough information.'\\n\"\n",
        "                f\"Answer:\"\n",
        "            )\n",
        "\n",
        "            # Tokenize with attention mask\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Generate answer (deterministic)\n",
        "            outputs = self.llm.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=20,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            raw_answer = answer[len(prompt):].strip()\n",
        "            print(f\"DEBUG: Raw generated answer: {raw_answer}\")\n",
        "\n",
        "            # Post-process with priority on extraction\n",
        "            query_lower = query.lower()\n",
        "            answer_text = None  # Start with None to force extraction check\n",
        "\n",
        "            if query_lower == \"who am i?\" and \"I am \" in context:\n",
        "                for line in context.split(\"\\n\"):\n",
        "                    if line.startswith(\"I am \"):\n",
        "                        answer_text = line[5:].strip(\".\").strip()\n",
        "                        print(f\"DEBUG: Extracted answer from context: {answer_text}\")\n",
        "                        break\n",
        "            elif query_lower.startswith(\"where\") and \"lives in\" in context:\n",
        "                for line in context.split(\"\\n\"):\n",
        "                    if \"lives in\" in line:\n",
        "                        answer_text = line.split(\"lives in\")[1].strip(\".\").strip()\n",
        "                        print(f\"DEBUG: Extracted answer from context: {answer_text}\")\n",
        "                        break\n",
        "            elif query_lower.startswith(\"what is\") and context:\n",
        "                answer_text = context.split(\".\")[0]\n",
        "                print(f\"DEBUG: Extracted answer from context: {answer_text}\")\n",
        "\n",
        "            # Fallback if no extraction occurred\n",
        "            if answer_text is None:\n",
        "                answer_text = \"I don’t have enough information.\"\n",
        "                print(f\"DEBUG: Fallback applied: {answer_text}\")\n",
        "            elif answer_text != raw_answer:\n",
        "                print(f\"DEBUG: Overriding raw answer '{raw_answer}' with extracted '{answer_text}'\")\n",
        "\n",
        "            # Store query and answer in history\n",
        "            self.history.append((query, answer_text))\n",
        "\n",
        "            return answer_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main loop for user interaction.\"\"\"\n",
        "    rag = RAGSystem()\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Ask a question\")\n",
        "        print(\"2. Add new documents\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            answer = rag.query(query)\n",
        "            if answer:\n",
        "                print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"Enter documents (one per line, empty line to finish):\")\n",
        "            new_docs = []\n",
        "            while True:\n",
        "                doc = input().strip()\n",
        "                if not doc:\n",
        "                    break\n",
        "                new_docs.append(doc)\n",
        "            if new_docs:\n",
        "                rag.add_documents(new_docs)\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4wYONJLOiWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}